{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "session2DSLab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi9qL0jUifUb",
        "colab_type": "text"
      },
      "source": [
        "# Triển khai thuật toán KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M9xleBiSAZ6",
        "colab_type": "text"
      },
      "source": [
        "## Lý thuyết\n",
        "Đầu vào :\n",
        "- Tập dữ liệu $R=\\{r_d :d\\in D\\}$ với $r_d \\in \\mathbb{R}^{|V|} $ là biểu diễn tf idf của d.\n",
        "- Số cụm K.\n",
        "\n",
        "Đầu ra:\n",
        "- $A= \\{a_d: d\\in D\\}$ với $a_d \\in \\{1,2,...,K\\}$ cho biết $d$ được phân vào cụm nào.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZjM5cvcjNfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKQD-gi0qY3_",
        "colab_type": "text"
      },
      "source": [
        "## Thực hiện Kmeans\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g4egDzJqplp",
        "colab_type": "code",
        "outputId": "106304f8-9dcf-4180-9edd-46e9a97d3d1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class Member:\n",
        "    def __init__(self, r_d, label=None, doc_id=None):\n",
        "        self._r_d = r_d\n",
        "        self._label = label\n",
        "        self._doc_id = doc_id\n",
        "\n",
        "\n",
        "class Cluster:\n",
        "    def __init__(self):\n",
        "        self._centroid = None\n",
        "        self._member = []\n",
        "\n",
        "    def reset_member(self):\n",
        "        self._member = []\n",
        "\n",
        "    def add_member(self, mem):\n",
        "        self._member.append(mem)\n",
        "\n",
        "\n",
        "class Kmeans:\n",
        "    def __init__(self, num_clusters):\n",
        "        self._num_cluster = num_clusters\n",
        "        self._cluster = [Cluster() for i in range(self._num_cluster)]\n",
        "        self._E = []  # tập các cụm\n",
        "        self._S = 0  # độ tương đồng\n",
        "\n",
        "    def load_data(self, path):\n",
        "        def sparse_to_dense(sparse_r_d, vocab_size):\n",
        "            '''hàm trích suất mảng các giá trị tf-idf'''\n",
        "            r_d = [0.0 for i in range(vocab_size)]\n",
        "            ind_tfidf = sparse_r_d.split()\n",
        "            for index_tfidf in ind_tfidf:\n",
        "                index = int(index_tfidf.split(\":\")[0])\n",
        "                tfidf = float(index_tfidf.split(':')[1])\n",
        "                r_d[index] = tfidf\n",
        "            return np.array(r_d)\n",
        "\n",
        "        with open(path) as f:\n",
        "            d_lines = f.read().splitlines()\n",
        "        with open('/content/drive/My Drive/Python code/DS_Lab/words-idfs.txt') as f:\n",
        "            vocab_size = len(f.read().splitlines())\n",
        "\n",
        "        self._data = []\n",
        "        self._label_count = defaultdict(int)\n",
        "        x=[]\n",
        "        label_data=[]\n",
        "        for data_id, d in enumerate(d_lines):\n",
        "            features = d.split('<fff>')\n",
        "            label, doc_id = int(features[0]), int(features[1])\n",
        "            self._label_count[label] += 1\n",
        "            r_d = sparse_to_dense(sparse_r_d=features[2], vocab_size=vocab_size)\n",
        "            # r_d là mảng các giá trị của văn bản d\n",
        "            self._data.append(Member(r_d=r_d, label=label, doc_id=doc_id))\n",
        "            \n",
        "            x.append(r_d)\n",
        "            label_data.append(label)\n",
        "        return (x, label_data)\n",
        "            # tạo mảng data có các Member đại diện cho 1 văn bản.\n",
        "\n",
        "    def random_init(self, seed_value):\n",
        "        # chọn ra k điểm tâm ban đầu từ tập seed_value\n",
        "        centre_init = random.sample(seed_value, self._num_cluster)\n",
        "        for i in range(self._num_cluster):\n",
        "            # gán cho từng điểm thành tâm của 1 cụm\n",
        "            self._cluster[i]._centroid = centre_init[i]\n",
        "\n",
        "    def random_init2(self):\n",
        "        X= np.array(self._data)\n",
        "        k= self._num_cluster\n",
        "        cluster_init= X[np.random.choice(X.shape[0], k, replace=False)]\n",
        "        # print(type(cluster_init[0]))\n",
        "        for i in range(self._num_cluster):\n",
        "            self._cluster[i]._centroid = cluster_init[i]\n",
        "\n",
        "\n",
        "    def compute_purity(self):\n",
        "        majority_sum = 0\n",
        "        for cluster in self._cluster:\n",
        "            member_labels = [member._label for member in cluster._member]\n",
        "            max_count = max([member_labels.count(label) for label in range(20)])\n",
        "            majority_sum += max_count\n",
        "        return majority_sum * 1. / len(self._data)\n",
        "\n",
        "    def compute_similarity(self, member, centroid):\n",
        "        # print(type(member))\n",
        "        # print(type(centroid))\n",
        "        a= member._r_d\n",
        "        b= centroid._r_d\n",
        "        return 1. / (np.linalg.norm(a - b) + 1)\n",
        "\n",
        "    def select_cluster_for(self, member):\n",
        "        # chọn tâm cụm cho từng member\n",
        "        best_fit_cluster = None\n",
        "        max_similarity = -1\n",
        "        for cluster in self._cluster:\n",
        "            similarity = self.compute_similarity(member, cluster._centroid)\n",
        "            if similarity > max_similarity:\n",
        "                best_fit_cluster = cluster\n",
        "                max_similarity = similarity\n",
        "        best_fit_cluster.add_member(member)\n",
        "        return max_similarity\n",
        "\n",
        "    def update_centroid_of(self, cluster):\n",
        "        member_r_ds = [member._r_d for member in cluster._member]\n",
        "        aver_r_d = np.mean(member_r_ds, axis=0)\n",
        "        sqrt_sum_sqr = np.sqrt(np.sum(aver_r_d ** 2))\n",
        "        new_centroid_r_d = np.array([value / sqrt_sum_sqr for value in aver_r_d])\n",
        "        new_centroid=Member(new_centroid_r_d, label=cluster._centroid._label, doc_id=cluster._centroid._doc_id )\n",
        "        cluster._centroid = new_centroid\n",
        "\n",
        "    def stopping_codition(self, criterion, threshold):\n",
        "        crits = ['centroid', 'similarity', 'max_iter']\n",
        "        assert criterion in crits\n",
        "\n",
        "        if criterion == 'max_iter':\n",
        "            # trong trường hợp chọn tiêu chuẩn dừng là số lần lặp\n",
        "            if self._iteration >= threshold:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "\n",
        "        elif criterion == 'centroid':\n",
        "            # chọn tiêu chuẩn dừng khi lượng cluster thay đổi nhở hơn ngưỡng\n",
        "            E_new = [list(cluster._centroid) for cluster in self._cluster]\n",
        "            E_new_minus_E = [centroid for centroid in E_new\n",
        "                             if centroid not in self._E]\n",
        "            self._E = E_new\n",
        "            if len(E_new_minus_E) <= threshold:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "\n",
        "        else:\n",
        "            # chọn tiêu chuẩn dừng khi độ lệch tương đồng (similarity) thay đổi\n",
        "            # nhỏ hơn 1 ngưỡng nào đó\n",
        "            self._new_S = 0\n",
        "            for member in self._data:\n",
        "                max_s = self.select_cluster_for(member)\n",
        "                self._new_S += max_s\n",
        "            new_S_minus_S = self._new_S - self._S\n",
        "            self._S = self._new_S\n",
        "            if new_S_minus_S <= threshold:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "\n",
        "    # def run(self, seed_value, criterion, threshold):\n",
        "    def run(self, criterion, threshold):\n",
        "        # chọn các tâm cụm đầu\n",
        "        # self.random_init(seed_value=seed_value)\n",
        "        self.random_init2()\n",
        "\n",
        "        # tiếp tục tiến hành cho đến khi hội tụ\n",
        "        self._iteration = 0\n",
        "        NMI_score=[]\n",
        "        while True:\n",
        "            # cập nhật lại các cluster;\n",
        "            for cluster in self._cluster:\n",
        "                cluster.reset_member()\n",
        "\n",
        "            self._new_S = 0\n",
        "            for member in self._data:\n",
        "                # gán các điểm vào các cluster\n",
        "                max_s = self.select_cluster_for(member)\n",
        "                self._new_S += max_s\n",
        "\n",
        "            for cluster in self._cluster:\n",
        "                self.update_centroid_of(cluster)\n",
        "\n",
        "            NMI_score.append(self.compute_NMI())\n",
        "            self._iteration += 1\n",
        "            if self.stopping_codition(criterion, threshold):\n",
        "                return np.array(NMI_score)\n",
        "\n",
        "    # Đánh giá chất lượng phân cụm bằng NMI\n",
        "    def compute_NMI(self):\n",
        "        I_value, H_omega, H_C, N = 0., 0., 0., len(self._data)\n",
        "        for cluster in self._cluster:\n",
        "            wk = len(cluster._member) * 1.\n",
        "            H_omega += -wk / N * np.log10(wk / N)\n",
        "            member_labels = [member._label for member in cluster._member]\n",
        "            for label in range(20):\n",
        "                wk_cj = member_labels.count(label) * 1.\n",
        "                cj = self._label_count[label]\n",
        "                I_value += wk_cj / N * np.log10(N * wk_cj / (wk * cj) + 1e-12)\n",
        "        for label in range(20):\n",
        "            cj = self._label_count[label] * 1.\n",
        "            H_C += -cj / N * np.log10(cj / N)\n",
        "        return I_value * 2. / (H_omega + H_C)\n",
        "\n",
        "\n",
        "\"\"\"## Thực hiện Kmeans \"\"\"\n",
        "\n",
        "# with open('./20news_bydate/words_idfs.txt') as f:\n",
        "#     vocab_size = len(f.read().splitlines())\n",
        "# print(vocab_size)\n",
        "\n",
        "k_means= Kmeans(20)\n",
        "k_means.load_data(\"/content/drive/My Drive/Python code/DS_Lab/full_tf_idf.txt\")\n",
        "\n",
        "iteration=np.arange(15)\n",
        "NMI_score=k_means.run(\"max_iter\",5)\n",
        "\n",
        "print(NMI_score[-1])\n",
        "print(k_means.compute_purity())\n",
        "\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(iteration, NMI_score)\n",
        "# plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.2237331584272206\n",
            "0.23973256924546324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ7dtHrLd_Am",
        "colab_type": "code",
        "outputId": "47d9eeae-0d45-486a-e683-09211e11ae81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install scipy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcDDuNYRpRQO",
        "colab_type": "text"
      },
      "source": [
        "## Thực hiện KMeans với Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq4HTyyaEq1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def load_data( path):\n",
        "  def sparse_to_dense(sparse_r_d, vocab_size):\n",
        "    '''hàm trích suất mảng các giá trị tf-idf'''\n",
        "    r_d = [0.0 for i in range(vocab_size)]\n",
        "    ind_tfidf = sparse_r_d.split()\n",
        "    for index_tfidf in ind_tfidf:\n",
        "                index = int(index_tfidf.split(\":\")[0])\n",
        "                tfidf = float(index_tfidf.split(':')[1])\n",
        "                r_d[index] = tfidf\n",
        "    return np.array(r_d)\n",
        "\n",
        "  with open(path) as f:\n",
        "    d_lines = f.read().splitlines()\n",
        "  with open('/content/drive/My Drive/Python code/DS_Lab/words-idfs.txt') as f:\n",
        "    vocab_size = len(f.read().splitlines())\n",
        "\n",
        "  # self._data = []\n",
        "  # self._label_count = defaultdict(int)\n",
        "  x=[]\n",
        "  label_data=[]\n",
        "  for data_id, d in enumerate(d_lines):\n",
        "    features = d.split('<fff>')\n",
        "    label, doc_id = int(features[0]), int(features[1])\n",
        "    # self._label_count[label] += 1\n",
        "    r_d = sparse_to_dense(sparse_r_d=features[2], vocab_size=vocab_size)\n",
        "    # r_d là mảng các giá trị của văn bản d\n",
        "    # self._data.append(Member(r_d=r_d, label=label, doc_id=doc_id))\n",
        "            \n",
        "    x.append(r_d)\n",
        "    label_data.append(label)\n",
        "  return x, label_data\n",
        "            # tạo mảng data có các Member đại diện cho 1 văn bản.\n",
        "\n",
        "def clustering_with_KMeans():\n",
        "  \n",
        "  data, label=load_data(\"/content/drive/My Drive/Python code/DS_Lab/full_tf_idf.txt\")\n",
        "  x= csr_matrix(data)\n",
        "  print(\"==================\")\n",
        "  kmeans= KMeans(\n",
        "      n_clusters=20,\n",
        "      init='random',\n",
        "      n_init= 5,\n",
        "      tol=1e-3,\n",
        "      random_state= 2020\n",
        "  ).fit(x)\n",
        "  labels= kmeans.labels_\n",
        "  print(labels)\n",
        "  \n",
        "clustering_with_KMeans()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1Gu019txWME",
        "colab_type": "text"
      },
      "source": [
        "## Practice with support vector machine in sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acRAkPNKxiA0",
        "colab_type": "code",
        "outputId": "b1f2d8f4-1e5d-445b-f2e0-72b12780983a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "def load_data( path):\n",
        "  def sparse_to_dense(sparse_r_d, vocab_size):\n",
        "    '''hàm trích suất mảng các giá trị tf-idf'''\n",
        "    r_d = [0.0 for i in range(vocab_size)]\n",
        "    ind_tfidf = sparse_r_d.split()\n",
        "    for index_tfidf in ind_tfidf:\n",
        "                index = int(index_tfidf.split(\":\")[0])\n",
        "                tfidf = float(index_tfidf.split(':')[1])\n",
        "                r_d[index] = tfidf\n",
        "    return np.array(r_d)\n",
        "\n",
        "  with open(path) as f:\n",
        "    d_lines = f.read().splitlines()\n",
        "  with open('/content/drive/My Drive/Python code/DS_Lab/words-idfs.txt') as f:\n",
        "    vocab_size = len(f.read().splitlines())\n",
        "\n",
        "  # self._data = []\n",
        "  # self._label_count = defaultdict(int)\n",
        "  x=[]\n",
        "  label_data=[]\n",
        "  for data_id, d in enumerate(d_lines):\n",
        "    features = d.split('<fff>')\n",
        "    label, doc_id = int(features[0]), int(features[1])\n",
        "    # self._label_count[label] += 1\n",
        "    r_d = sparse_to_dense(sparse_r_d=features[2], vocab_size=vocab_size)\n",
        "    # r_d là mảng các giá trị của văn bản d\n",
        "    # self._data.append(Member(r_d=r_d, label=label, doc_id=doc_id))\n",
        "            \n",
        "    x.append(r_d)\n",
        "    label_data.append(label)\n",
        "  return np.array(x), np.array(label_data)\n",
        "\n",
        "def compute_accuracy(predicted_y, test_y):\n",
        "  matchs= np.equal(predicted_y, test_y)\n",
        "  accuracy= np.sum(matchs.astype(float)/ test_y.size)\n",
        "  return accuracy\n",
        "\n",
        "def classifying_with_linear_SVMs():\n",
        "  train_x, train_y= load_data(\"/content/drive/My Drive/Python code/DS_Lab/train_tf_idf.txt\")\n",
        "  classifier= LinearSVC(\n",
        "      C=10.0,\n",
        "      tol=0.001,\n",
        "      verbose= True\n",
        "  )\n",
        "\n",
        "  classifier.fit(train_x,train_y)\n",
        "  test_x, test_y= load_data('/content/drive/My Drive/Python code/DS_Lab/test_tf_idf.txt')\n",
        "  predicted_y= classifier.predict(test_x)\n",
        "  accuracy= compute_accuracy(predicted_y, test_y)\n",
        "  print(\"accuracy: {}\".format(accuracy))\n",
        "\n",
        "# def classifying_with_linear_SVC():\n",
        "#   train_x, train_y= load_data(\"/content/drive/My Drive/Python code/DS_Lab/train_tf_idf.txt\")\n",
        "#   classifier= SVC(\n",
        "#       C=50.0,\n",
        "#       kernel=\"rbf\",\n",
        "#       gamma= 0.1,\n",
        "#       tol=0.001,\n",
        "#       verbose= True\n",
        "#   )\n",
        "\n",
        "#   classifier.fit(train_x,train_y)\n",
        "#   test_x, test_y= load_data('/content/drive/My Drive/Python code/DS_Lab/test_tf_idf.txt')\n",
        "#   predicted_y= classifier.predict(test_x)\n",
        "#   accuracy= compute_accuracy(predicted_y, test_y)\n",
        "#   print(\"accuracy: {}\".format(accuracy))\n",
        "  \n",
        "classifying_with_linear_SVMs()\n",
        "# classifying_with_linear_SVC()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibLinear]accuracy: 0.8155868295273498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVCNftr5zOZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}